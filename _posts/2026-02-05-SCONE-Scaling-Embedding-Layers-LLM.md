---
layout: post
title: SCONE - 언어 모델의 임베딩 레이어 확장 기법
author: 'Juho'
date: 2026-02-05 02:00:00 +0900
categories: [AI]
tags: [AI, LLM]
pin: True
toc: True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }
</style>

## 목차
1. [개요](#개요)
2. [연구 배경](#연구-배경)
3. [SCONE 방법론](#scone-방법론)
4. [핵심 기술 상세](#핵심-기술-상세)
5. [실험 결과](#실험-결과)
6. [시사점 및 적용 가능성](#시사점-및-적용-가능성)
7. [Reference](#reference)

## 개요

Google Research 연구팀이 NeurIPS 2025에서 발표한 "SCONE: Scaling Embedding Layers in Language Models" 논문을 소개한다.
이 연구는 대규모 언어 모델에서 임베딩 레이어의 효율적인 확장 방법을 제안한다.
SCONE(Scalable, Contextualized, Offloaded, N-gram Embedding)은 디코딩 비용 증가 없이 모델 성능을 향상시키는 새로운 접근법이다.

핵심 성과는 1B 파라미터 모델이 1.9B 기준선 모델을 능가하면서, 추론 시 약 절반의 FLOPS와 메모리만 사용한다는 점이다.

## 연구 배경

### 기존 임베딩 레이어의 한계

대규모 언어 모델에서 임베딩 레이어는 상당한 메모리와 계산 비용을 차지한다.
모델 규모가 커질수록 임베딩 레이어의 비효율성이 더욱 두드러진다.
기존 방식은 어휘 크기 증가나 임베딩 차원 확대 시 리소스 소비가 선형적으로 증가하는 문제가 있다.

### 연구 목표

이 연구는 다음 질문에 답하고자 한다.

- 임베딩 레이어를 어떻게 효율적으로 확장할 수 있는가?
- 추론 비용 증가 없이 모델 성능을 향상시킬 수 있는가?
- 메모리와 계산 효율성을 동시에 달성할 수 있는가?

## SCONE 방법론

### 핵심 아이디어

SCONE의 핵심은 기존 어휘를 유지하면서 자주 사용되는 n-gram의 임베딩을 추가하는 것이다.
이 접근법은 디코딩 비용을 증가시키지 않으면서 입력 표현력을 향상시킨다.

### 주요 구성 요소

| 구성 요소 | 설명 |
|-----------|------|
| N-gram 임베딩 | 자주 등장하는 토큰 시퀀스에 대한 임베딩 추가 |
| 오프로드 저장 | 임베딩을 가속기 외부 메모리에 저장 |
| 사전 계산 | 학습 완료 후 임베딩을 미리 계산하여 저장 |
| 최소 지연 설계 | 임베딩 조회의 낮은 복잡도로 추론 영향 최소화 |

### SCONE의 의미

SCONE은 다음 약어의 조합이다.

- **S**calable: 확장 가능한
- **C**ontextualized: 문맥화된
- **O**ffloaded: 오프로드된
- **N**-gram: N-gram 기반
- **E**mbedding: 임베딩

## 핵심 기술 상세

### N-gram 임베딩 확장

기존 토큰 임베딩에 더해 자주 등장하는 n-gram 시퀀스에 대한 별도의 임베딩을 학습한다.
이를 통해 모델이 토큰 수준을 넘어 구문 수준의 패턴을 더 효과적으로 포착할 수 있다.

예를 들어 "machine learning"이라는 빈번한 2-gram에 대해 개별 토큰 임베딩 외에 추가 임베딩을 학습한다.
이 방식은 자주 함께 등장하는 토큰 조합의 의미를 더 정확하게 표현한다.

### 오프로드 메모리 활용

SCONE의 핵심 혁신 중 하나는 임베딩을 가속기(GPU/TPU) 외부 메모리에 저장하는 것이다.

기존 방식의 문제점:
- 모든 임베딩이 가속기 메모리에 상주
- 메모리 제약으로 임베딩 확장에 한계

SCONE의 해결책:
- 학습 완료 후 임베딩을 CPU 메모리나 SSD에 저장
- 필요한 임베딩만 실시간으로 조회
- 가속기 메모리는 모델 연산에 집중 활용

### 사전 계산 전략

학습 단계에서 n-gram 임베딩을 완전히 학습한 후, 추론 전에 모든 임베딩을 미리 계산하여 저장한다.
이를 통해 추론 시 임베딩 계산 없이 단순 조회만 수행하여 지연 시간을 최소화한다.

## 실험 결과

### 성능 비교

논문의 핵심 실험 결과는 다음과 같다.

| 모델 | 파라미터 수 | 상대 성능 |
|------|-------------|-----------|
| 기준선 모델 | 1.9B | 100% |
| SCONE 모델 | 1B | 기준선 초과 |

SCONE 적용 시 1B 파라미터 모델이 1.9B 기준선 모델보다 높은 성능을 달성했다.
이는 거의 2배의 파라미터 효율성 향상을 의미한다.

### 리소스 효율성

| 지표 | SCONE 대비 기준선 |
|------|------------------|
| 추론 FLOPS | 약 50% 절감 |
| 가속기 메모리 | 약 50% 절감 |

SCONE은 추론 중 필요한 계산량과 메모리 사용량을 모두 절반 수준으로 줄였다.
이는 동일한 하드웨어에서 더 많은 요청을 처리하거나, 더 저렴한 하드웨어로 동일 성능을 달성할 수 있음을 의미한다.

### 확장 전략

논문에서는 두 가지 확장 전략을 제시한다.

1. **N-gram 임베딩 수 증가**: 더 많은 n-gram 패턴에 대한 임베딩을 추가하여 표현력 향상
2. **학습 모델 확장**: n-gram 임베딩을 학습하는 모델 자체의 크기를 키워 임베딩 품질 향상

두 전략을 조합하여 최적의 성능-효율성 균형점을 찾을 수 있다.

## 시사점 및 적용 가능성

### 산업적 의의

SCONE은 LLM 배포 비용 절감에 직접적인 영향을 미칠 수 있다.

- 동일 하드웨어에서 2배 수준의 처리량 달성 가능
- 클라우드 비용 절감 또는 온프레미스 배포 용이
- 엣지 디바이스에서의 LLM 실행 가능성 확대

### 연구적 의의

이 연구는 임베딩 레이어 설계에 대한 새로운 관점을 제시한다.

기존에는 모델 아키텍처(어텐션, FFN 등)에 집중했다면, 이 연구는 임베딩 레이어의 최적화 가능성을 보여준다.
특히 메모리 계층 구조를 활용한 효율화는 향후 연구에 중요한 방향성을 제시한다.

### 적용 시 고려사항

SCONE 적용 시 다음 사항을 고려해야 한다.

- N-gram 선정 기준: 어떤 n-gram을 임베딩으로 추가할지 결정 필요
- 오프로드 메모리 접근 지연: CPU 메모리나 SSD 접근 시간 고려
- 학습 비용: 추가 임베딩 학습에 따른 학습 시간 증가
- 도메인 특화: 특정 도메인에서 빈번한 n-gram이 다를 수 있음

### 향후 발전 방향

SCONE의 아이디어는 다음과 같이 확장될 수 있다.

- 동적 n-gram 선정: 입력에 따라 적응적으로 n-gram 임베딩 활용
- 다국어 모델 적용: 언어별 특화된 n-gram 임베딩 학습
- 멀티모달 확장: 텍스트 외 다른 모달리티에 유사 기법 적용

## Reference

- [SCONE: Scaling Embedding Layers in Language Models - arXiv](https://arxiv.org/abs/2502.01637)
