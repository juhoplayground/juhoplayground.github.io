---
layout: post
title: "에이전틱 코딩 벤치마크에서 인프라 노이즈 정량화 : Anthropic 엔지니어링 분석"
author: 'Juho'
date: 2026-02-13 02:00:00 +0900
categories: [AI]
tags: [AI, Benchmark]
pin: True
toc: True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }
</style>

## 목차
1. [개요](#개요)
2. [핵심 발견](#핵심-발견)
3. [문제의 본질](#문제의-본질)
4. [문제 발견 과정](#문제-발견-과정)
5. [실험 설계와 결과](#실험-설계와-결과)
6. [3배 헤드룸의 의미](#3배-헤드룸의-의미)
7. [실제 사례 : bn-fit-modify 태스크](#실제-사례--bn-fit-modify-태스크)
8. [교차 벤치마크 검증](#교차-벤치마크-검증)
9. [기타 변동 요인](#기타-변동-요인)
10. [권고사항](#권고사항)
11. [시사점](#시사점)
12. [Reference](#reference)

## 개요

Anthropic 엔지니어링 팀이 에이전틱 코딩 벤치마크에서 인프라 설정 차이가 결과에 미치는 영향을 정량적으로 분석한 결과를 발표했다.
인프라 리소스 설정 차이만으로 Terminal-Bench 2.0 벤치마크 점수가 최대 6 퍼센트포인트까지 변동할 수 있다는 사실을 밝혀냈다.
이는 때로 리더보드 상위 모델 간의 점수 차이보다 더 큰 수치이다.
이 글에서는 Anthropic이 어떻게 이 문제를 발견하고 실험적으로 검증했는지를 살펴본다.

## 핵심 발견

인프라 설정 차이가 벤치마크 점수를 최대 6 퍼센트포인트까지 변동시킬 수 있다.
이 수치는 리더보드에서 최상위 모델들 사이의 격차보다 더 클 수 있다.
즉, 리더보드에서의 경쟁 우위가 실제 모델 능력의 차이가 아니라 단순히 더 큰 VM을 사용한 결과일 수 있다.

## 문제의 본질

SWE-bench나 Terminal-Bench 같은 에이전틱 코딩 벤치마크는 정적 평가와 근본적으로 다르다.
모델 출력을 직접 채점하는 대신, 모델이 코드를 작성하고 테스트를 실행하며 의존성을 설치하고 반복하는 전체 환경을 제공한다.
이는 런타임 리소스가 결과에 영향을 미치는 통합 구성 요소가 된다는 것을 의미한다.
리소스는 수동적 컨테이너가 아니라 실험 결과의 일부가 되는 것이다.

## 문제 발견 과정

Anthropic 팀은 Google Kubernetes Engine에서 Terminal-Bench 2.0을 실행하던 중 공식 리더보드 결과와의 불일치를 발견했다.
인프라 오류율이 6%에 달했으며, 모델 능력과 무관한 리소스 제약으로 파드(pod)가 실패하고 있었다.

근본 원인은 Kubernetes 구현이 태스크별 리소스 사양을 바닥(floor)과 천장(ceiling)으로 동시에 적용한 것이었다.
컨테이너 런타임은 보장 할당량(guaranteed allocation)과 강제 종료 임계값(hard kill threshold)이라는 두 가지 파라미터를 사용한다.
이 둘을 동일하게 설정하면 일시적 스파이크를 위한 여유 공간이 전혀 남지 않게 된다.

## 실험 설계와 결과

Anthropic은 동일한 Claude 모델, 하네스, 태스크 세트를 사용하여 Terminal-Bench 2.0을 여섯 가지 리소스 설정에서 테스트했다.

### 리소스 설정

| 설정 | 설명 |
|------|------|
| 1x (엄격 적용) | 사양 제한에서 하드 실링 |
| 3x 헤드룸 | 태스크별 사양의 3배 |
| 무제한(Uncapped) | 리소스 제한 없음 |

### 인프라 오류율

| 설정 | 오류율 |
|------|--------|
| 1x | 5.8% |
| 3x | 2.1% (p value 0.001 미만) |
| 무제한 | 0.5% |

### 성공률 변화

| 비교 구간 | 성공률 변화 | 통계적 유의성 |
|-----------|------------|---------------|
| 1x에서 3x | 미미함 | p=0.40 |
| 3x에서 무제한 | +4 퍼센트포인트 | 유의함 |
| 1x에서 무제한 | +6 퍼센트포인트 | p value 0.01 미만 |

1x에서 3x로의 전환은 성공률에 거의 영향을 미치지 않았다.
1x에서 실패하는 태스크는 리소스와 무관하게 어차피 실패하는 것이었다.
그러나 3x에서 무제한으로 전환하면 4 퍼센트포인트, 1x에서 무제한으로 전환하면 6 퍼센트포인트의 성공률 향상이 나타났다.

## 3배 헤드룸의 의미

약 3배까지의 리소스 할당에서는 추가 리소스가 주로 인프라 안정성 문제를 해결한다.
평가가 더 안정적으로 되지만 더 쉬워지지는 않는 것이다.

3배를 넘어서면 추가 리소스가 다른 문제 해결 전략을 적극적으로 가능하게 한다.
에이전트가 대형 의존성을 가져오고, 비용이 큰 서브프로세스를 생성하며, 메모리 집약적인 테스트 스위트를 실행할 수 있게 된다.
이는 타이트한 제약 조건에서는 불가능했던 전략이다.

이 구분은 중요한 의미를 가진다.
3배까지는 "평가 안정화"이고, 3배를 넘어서면 "평가 조건 변경"이 되는 것이다.

## 실제 사례 : bn-fit-modify 태스크

베이지안 네트워크 피팅 태스크인 bn-fit-modify는 이 효과를 잘 보여준다.

일부 모델의 기본 접근 방식은 표준 Python 데이터 과학 스택(pandas, networkx, scikit-learn)을 설치하는 것이다.
넉넉한 리소스 제한에서는 이 접근이 성공한다.
하지만 타이트한 제한에서는 설치 중에 파드의 메모리가 부족해진다.

더 가벼운 전략도 존재한다.
표준 라이브러리만 사용하여 수학을 직접 구현하는 방식이다.
서로 다른 모델은 서로 다른 접근 방식을 기본으로 사용하며, 리소스 설정이 어떤 접근이 성공할지를 결정한다.

이는 벤치마크가 "모델의 코딩 능력"을 측정하는 것인지, "모델이 특정 리소스 환경에서 적합한 전략을 선택하는 능력"을 측정하는 것인지에 대한 근본적인 질문을 제기한다.

## 교차 벤치마크 검증

Anthropic은 이 가설을 SWE-bench에서도 검증했다.
227개 문제에 대해 각 10개 샘플로 가용 RAM을 기준 대비 최대 5배까지 변화시켰다.
동일한 단조 증가 패턴이 나타났다.
크기는 더 작았지만(5x 대 1x에서 1.54 퍼센트포인트), 패턴이 에이전틱 평가 전반에 걸쳐 널리 적용됨을 확인했다.

## 기타 변동 요인

시간대별 변동도 결과에 영향을 미친다.
트래픽 패턴에 따른 API 지연 시간 변동이 원인으로 추정된다.
종단 간(end-to-end) 시스템 테스트에서 "모델 능력"과 "인프라 동작"의 경계는 여전히 모호하다.

모델 제공자는 전용 하드웨어를 통해 인프라를 차폐할 수 있다.
하지만 외부 평가자는 이러한 교란 요인을 쉽게 통제할 수 없다.

### 통계적 맥락

중간 수준의 리소스 설정 간 관측된 편차는 약 2 퍼센트포인트 미만이다.
단순 이항 신뢰 구간도 이미 1~2 퍼센트포인트에 걸쳐 있다.
인프라 교란 요인은 이 범위 위에 추가로 쌓이며, 극단적인 경우 편차가 6 퍼센트포인트에 달한다.

## 권고사항

### 벤치마크 설계자를 위한 권고

고정 값 대신 태스크별로 리소스의 바닥(floor)과 천장(ceiling) 두 파라미터를 별도로 지정해야 한다.
바닥과 천장 사이의 범위를 조정하여 점수가 통계적 노이즈 내에 들어오도록 해야 한다.
Terminal-Bench 2.0의 경우 3배 천장이 인프라 오류를 5.8%에서 2.1%로 줄이면서 점수 상승을 노이즈 수준(p=0.40)으로 유지했다.
사용된 정확한 배수를 보고해야 하며, 이는 벤치마크와 태스크 분포에 따라 달라진다.

### 연구소를 위한 권고

리소스 설정을 프롬프트 형식이나 샘플링 온도와 동일한 수준의 엄밀함으로 문서화되는 일급 실험 변수로 취급해야 한다.

### 벤치마크 관리자를 위한 권고

권장 사양을 공개하는 것(Terminal-Bench 2.0이 하고 있는 것처럼)은 도움이 된다.
적용 방법론(enforcement methodology)까지 명시하면 나머지 차이를 해소할 수 있다.

### 결과 소비자를 위한 권고

3 퍼센트포인트 미만의 리더보드 차이는 평가 설정이 문서화되고 일치될 때까지 회의적으로 봐야 한다.

## 시사점

에이전틱 평가에서 작은 점수 차이는 보고된 정밀도가 시사하는 것보다 훨씬 더 큰 불확실성을 내포한다.
리소스 방법론 표준화가 이루어지기 전까지, 리더보드에서의 경쟁 우위는 진정한 능력 차이를 반영하는 것일 수도 있고, 단순히 다른 하드웨어 설정의 결과일 수도 있다.
Anthropic의 이 연구는 벤치마크 결과를 해석할 때 인프라 설정이라는 숨겨진 변수를 반드시 고려해야 한다는 점을 실증적으로 보여준다.

## Reference

- [Quantifying Infrastructure Noise in Agentic Coding Evals - Anthropic Engineering](https://www.anthropic.com/engineering/infrastructure-noise)
