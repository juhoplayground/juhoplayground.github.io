---
layout: post
title: 추론 모델의 환상
author: 'Juho'
date: 2025-06-11 09:00:00 +0900
categories: [AI]
tags: [AI, LLM]
pin: True
toc : True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }

</style>

AI의 추론 기능을 어디까지 믿을 수 있을까?  
빅테크 기업들의 Large Reasoning Model (LRM)은 성능 향상을 자랑했지만  
실제로 사용해보면 응답 속도가 느려지는것에 비해 정답률은 기대 이하인 경우가 많았다.  
최근 [Apple의 AI 연구팀의 논문](https://machinelearning.apple.com/research/illusion-of-thinking){:target="_blank"}도 이 점에 대해 지적하였다. 

## Apple 연구팀의 발견
  
대상 모델 : 모델: Claude 3.7, DeepSeek-R1, OpenAI o-series  
방법 : 난이도를 달리한 퍼즐 풀이 테스트  

| **복잡도 구간**   | **성능 경향**                                           |
|------------------|--------------------------------------------------------|
| **낮은 난이도**   | 일반 LLM이 오히려 LRM보다 정확도 우위                    |
| **중간 난이도**   | LRM이 추가적 ‘사고 과정’ 덕분에 우세                     |
| **높은 난이도**   | 두 모델 모두 정확도 완전 붕괴                            |

현상  
1) 난이도가 일정 수준을 넘어서면 일반화 가능한 추론 능력 미형성  
2) 토큰이 충분해도 일정 지점 이후 추론 과정이 오히려 줄어듦(LRM에 내재적 연산 스케일 한계가 있음)    
3) 단순 문제에서는 over-thinking, 복잡 문제에서는 완전 실패라는 복잡도 의존 패턴 확인  

###  실험의 한계점  
- 도메인 편협성 :  퍼즐 기반 환경이 실제 세계 과제를 충분히 대변하지 못할 수 있음  
- 블랙박스 접근 : 폐쇄형 API라 내부 아키텍처 분석이 제한적  
- 완전 검증 가정 : 구조화가 낮은 영역에선 정밀 검증 자체가 어려움  

### 실험의 시사점  
**사고의 모방**이라는 구조적 한계  
현재 LLM은 언어를 확률적으로 예측할 뿐 의미를 깊이 이해하거나 논리적으로 사고하지 못한다.  
정확 계산의 한계: 하노이 탑의 해법 알고리즘을 모델에 제공해도 성능이 개선되지 않았다.  
첫 실패 지점 분석: 하노이 탑에서는 최대 100회까지 올바른 수를 두면서도 강 건너기 퍼즐에서는 5회도 넘기지 못했다.
현 접근법이 AGI로 나아가는 데 근본적 장벽에 직면했음을 시사한다.  
  
  

---  

AGI 단계 중 2단계(Reasoning)가 저물고 3단계(Agent)의 시대가 맞구나 하는 생각이 들었다.  
그렇기에 이제는 값 비싼 추론 모델을 계속 사용하기 보다는 목적별로 모델을 재구성하고  
단순히 AI에게 "생각해서 처리해봐"가 아닌 사람이 먼저 "논리 구조를 설계해서 처리하도록" 해야할 것 같다.  
잘 설계된 워크 플로우와 에이전트가 필요한 시기인 것 같다.  