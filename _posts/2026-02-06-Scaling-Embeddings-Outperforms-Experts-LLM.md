---
layout: post
title: 임베딩 확장이 전문가 확장보다 우수하다 - LLM 아키텍처의 새로운 방향
author: 'Juho'
date: 2026-02-06 00:00:00 +0900
categories: [AI]
tags: [AI, LLM]
pin: True
toc: True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }
</style>

## 목차
1. [개요](#개요)
2. [연구 배경](#연구-배경)
3. [N-gram Embedding Layer 방법론](#n-gram-embedding-layer-방법론)
4. [실험 설정](#실험-설정)
5. [LongCat-Flash-Lite 모델](#longcat-flash-lite-모델)
6. [실험 결과](#실험-결과)
7. [시사점 및 향후 전망](#시사점-및-향후-전망)
8. [Reference](#reference)

## 개요

"Scaling Embeddings Outperforms Scaling Experts in Language Models" 논문은 대규모 언어 모델 확장에 대한 새로운 관점을 제시한다.
이 연구는 2026년 1월 arXiv에 공개되었다.

기존 Mixture-of-Experts(MoE) 아키텍처의 한계를 지적하며, 임베딩 확장이 더 효율적인 대안이 될 수 있음을 실증적으로 보여준다.
핵심 주장은 특정 영역에서 임베딩 확장이 전문가 확장보다 우수한 파레토 경계(Pareto frontier)를 달성한다는 것이다.

## 연구 배경

### MoE 아키텍처의 한계

Mixture-of-Experts(MoE)는 현재 대규모 언어 모델 확장의 표준 방식으로 자리 잡았다.
MoE는 여러 전문가 네트워크 중 일부만 활성화하여 계산 효율성을 높이는 방식이다.
그러나 MoE 아키텍처는 점점 다음과 같은 문제에 직면하고 있다.

| 문제 | 설명 |
|------|------|
| 수익 감소 | 전문가 수 증가에 따른 성능 향상 폭이 점점 줄어듦 |
| 시스템 병목 | 전문가 라우팅과 통신 오버헤드 증가 |
| 복잡성 증가 | 학습 및 추론 시스템의 복잡도 상승 |

### 새로운 접근: 임베딩 확장

이 연구는 임베딩 확장을 희소성 확장의 직교(orthogonal) 차원으로 제안한다.
기존에는 전문가 네트워크 수를 늘리는 방식이 주류였다면, 이 연구는 임베딩 레이어의 확장에 주목한다.
임베딩 확장은 MoE와 완전히 다른 차원에서 모델 용량을 늘리는 방식이다.

### 비교 대상 기술

논문에서는 다음 기술들과 비교 분석을 수행했다.

- GShARD: 전통적인 전문가 확장 방식
- Apple의 확장 방법론
- LongCat-Flash 등 기존 경쟁 방식들

## N-gram Embedding Layer 방법론

### 핵심 아이디어

논문에서 제시하는 핵심 기술은 N-gram Embedding Layer이다.
기존 임베딩 계층을 단일 고차원 벡터로 생각하는 대신, 구조화된 방식으로 확장한다.
이 접근법은 CANINE 및 바이트 레벨 처리 방식에서 영감을 받았지만, 효율성과 확장성을 개선한 형태이다.

### 주요 구성 요소

| 구성 요소 | 설명 |
|-----------|------|
| N-gram 기반 임베딩 | 연속된 토큰 시퀀스에 대한 임베딩 계산 |
| 효율적 파라미터 공유 | 임베딩 간 파라미터 공유로 메모리 효율성 확보 |
| 구조화된 확장 | 단순 차원 증가가 아닌 구조적 확장 |

### 기술적 혁신 포인트

**1. 어휘 임베딩 최적화**

과도하게 토큰화된 어휘 문제를 해결한다.
기존 방식에서는 어휘 크기 증가가 곧 메모리 증가로 이어졌다.
N-gram 임베딩은 이 문제를 구조적으로 해결한다.

**2. 차원 효율성**

바이트/서브워드 임베딩을 개선하여 차원당 정보량을 높인다.
동일한 차원에서 더 풍부한 표현이 가능하다.

**3. 조건부 계산**

불필요한 계산을 회피하여 추론 효율성을 높인다.
모든 임베딩을 항상 계산하지 않고, 필요한 부분만 선택적으로 계산한다.

## 실험 설정

### 평가 벤치마크

논문은 다양한 벤치마크를 사용하여 종합적인 평가를 수행했다.

**언어 이해 능력 평가**

| 벤치마크 | 설명 |
|----------|------|
| MMLU | 일반 상식 다중선택형 평가 |
| MMLU-Pro | MMLU의 강화된 버전 |
| C-Eval | 중국어 평가 데이터셋 |
| CMMLU | 중국어 다중선택형 평가 |
| BBH | Big Bench Hard - 어려운 과제들 |

**추론 및 수학 능력**

| 벤치마크 | 설명 |
|----------|------|
| GSM8K | 초등학교 수준 수학 문제 해결 |
| MATH-500 | 고난도 수학 문제 |
| AIME 2024 | 미국 수학 올림피아드 2024 |
| AIME 2025 | 미국 수학 올림피아드 2025 |

**코딩 능력**

| 벤치마크 | 설명 |
|----------|------|
| HumanEval-MBPP+ | 코드 생성 평가 |
| BigCodeBench | 대규모 코딩 벤치마크 |
| SWE-Bench | 소프트웨어 엔지니어링 작업 |

**지식 및 기타 평가**

| 벤치마크 | 설명 |
|----------|------|
| GPQA | 대학원 수준 질문 응답 |
| Super-GPQA | GPQA 확장 버전 |
| DROP | 판독 이해도 평가 |
| Tau2-Bench | 추론 능력 평가 |
| ViTaBench | 비전-언어 평가 |
| Terminal-Bench | 에이전트 성능 평가 |

## LongCat-Flash-Lite 모델

### 모델 개요

연구팀은 제안한 방법론을 적용하여 LongCat-Flash-Lite 모델을 개발했다.

| 항목 | 수치 |
|------|------|
| 전체 매개변수 | 68.5B |
| 활성 매개변수 | 약 3B |
| 임베딩 매개변수 | 30B 이상 |

전체 68.5B 매개변수 중 실제로 추론 시 활성화되는 것은 약 3B에 불과하다.
30B 이상의 매개변수가 임베딩에 할당되어 있지만, 추론 시 계산 비용은 낮게 유지된다.

### 성능 특징

LongCat-Flash-Lite는 다음과 같은 성능을 보여준다.

- 동등 규모의 MoE 기준선 모델을 능가
- 에이전트 작업에서 경쟁력 있는 성능
- 코딩 작업에서 경쟁력 있는 성능

특히 에이전트와 코딩 작업에서 좋은 성능을 보인다는 점이 주목할 만하다.
이러한 작업들은 최근 LLM 활용에서 핵심적인 영역이기 때문이다.

## 실험 결과

### 주요 발견

연구팀은 포괄적인 분석과 실험을 통해 다음을 발견했다.

**1. 파레토 우위**

특정 영역에서 임베딩 확장이 전문가 확장보다 우수한 파레토 경계를 달성한다.
파레토 경계란 하나의 지표를 개선하려면 다른 지표가 악화되는 최적점의 집합을 의미한다.
임베딩 확장이 성능-효율성 측면에서 더 나은 트레이드오프를 제공한다.

**2. 아키텍처 요소의 영향**

효율성을 좌우하는 핵심 아키텍처 요소들을 분석했다.

| 요소 | 영향 |
|------|------|
| 매개변수 예산 | 전체 모델 크기 대비 임베딩 할당 비율 |
| 모델 너비 | 히든 레이어의 차원 크기 |
| 모델 깊이 | 트랜스포머 레이어 수 |

**3. 실제 추론 속도 개선**

시스템 최적화와 추측 디코딩(speculative decoding)을 통해 실제 추론 속도 향상을 달성했다.
이론적 효율성뿐 아니라 실제 배포 환경에서도 이점이 있음을 검증했다.

### 비교 분석

연구팀은 임베딩 확장 모델과 기존 MoE 모델을 다양한 조건에서 비교했다.

**효율성 측면**

- 임베딩 확장 모델이 동일 활성 매개변수에서 더 높은 성능 달성
- 메모리 사용량 대비 성능 비율에서 우위
- 파라미터당 더 높은 성능 향상

**실용성 측면**

- 추측 디코딩과의 호환성 우수
- 시스템 최적화 적용 용이
- 다양한 벤치마크에서 안정적 개선

### 벤치마크별 성능

| 벤치마크 카테고리 | LongCat-Flash-Lite 성능 |
|------------------|------------------------|
| 언어 이해 (MMLU 등) | MoE 기준선 능가 |
| 수학 추론 (GSM8K, MATH) | 경쟁력 있음 |
| 코딩 (HumanEval, SWE-Bench) | 경쟁력 있음 |
| 에이전트 (Terminal-Bench) | 경쟁력 있음 |

## 시사점 및 향후 전망

### 아키텍처 설계의 새로운 관점

이 연구는 LLM 아키텍처 설계에 새로운 관점을 제시한다.

기존 관점:
- 모델 확장 = 전문가 수 증가 (MoE)
- 또는 모델 확장 = 레이어/차원 증가 (Dense)

새로운 관점:
- 임베딩 레이어를 희소성 확장의 또 다른 축으로 고려
- 작업 특성에 따라 최적의 확장 전략 선택

### 실무적 의의

**배포 효율성**

- 68.5B 모델에서 3B만 활성화되므로 추론 비용 대폭 절감
- 대규모 임베딩은 메모리에 저장하고 필요시 조회

**시스템 단순화**

- MoE의 복잡한 라우팅 메커니즘 불필요
- 전문가 간 통신 오버헤드 제거

### 제한사항

논문에서 언급한 제한사항은 다음과 같다.

- 특정 모델 크기 범위에서의 평가
- 특정 언어/도메인에 대한 일반화 가능성
- 추론 속도 vs 정확도 트레이드오프

### 향후 연구 방향

이 연구는 다음과 같은 후속 연구 방향을 제시한다.

1. **하이브리드 접근**: 임베딩 확장과 MoE를 결합한 아키텍처 탐구
2. **도메인 특화**: 특정 도메인에 최적화된 임베딩 확장 전략
3. **효율적 학습**: 대규모 임베딩의 효율적 학습 방법 개발
4. **동적 임베딩**: 입력에 따라 적응적으로 임베딩을 활용하는 방법

### 결론

임베딩 확장은 MoE에 대한 유망한 대안 또는 보완책이 될 수 있다.
특히 수익 감소와 시스템 복잡성 문제에 직면한 상황에서 새로운 돌파구를 제공한다.
LongCat-Flash-Lite의 성공은 이 접근법의 실용성을 입증한다.
향후 대규모 언어 모델 개발에서 임베딩 층에 더 많은 관심을 기울여야 함을 시사한다.

## Reference

- [Scaling Embeddings Outperforms Scaling Experts in Language Models - arXiv](https://arxiv.org/abs/2601.21204)
