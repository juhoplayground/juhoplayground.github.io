---
layout: post
title: Engram - 조건부 메모리 검색을 통한 LLM의 새로운 희소성 축
author: 'Juho'
date: 2026-01-31 00:00:00 +0900
categories: [AI]
tags: [AI, LLM]
pin: True
toc: True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }
</style>

## 목차
1. [개요](#개요)
2. [문제 제기](#문제-제기)
3. [Engram 모듈 소개](#engram-모듈-소개)
4. [기술적 혁신](#기술적-혁신)
5. [성능 개선 결과](#성능-개선-결과)
6. [하드웨어 친화적 설계](#하드웨어-친화적-설계)
7. [의의 및 결론](#의의-및-결론)
8. [Reference](#reference)

---

## 개요

Xin Cheng 등 14명의 연구자가 발표한 이 논문은 대규모 언어 모델(LLM)에 새로운 형태의 희소성(Sparsity)을 도입하는 Engram 모듈을 제안합니다.
Mixture-of-Experts(MoE) 방식이 조건부 계산을 통해 모델 용량을 확장하는 것처럼, Engram은 효율적인 지식 검색을 위한 새로운 메커니즘을 제공합니다.

---

## 문제 제기

### Transformer의 한계

현재 Transformer 아키텍처는 지식 검색을 위한 기본 메커니즘이 부족합니다.
모든 지식이 모델 파라미터에 암묵적으로 저장되어 있어, 특정 지식에 접근할 때 전체 네트워크를 통과해야 합니다.

### MoE의 성공과 그 너머

Mixture-of-Experts(MoE)는 조건부 계산을 통해 모델 용량을 효과적으로 확장했습니다.
하지만 MoE는 계산적 희소성에 초점을 맞추고 있으며, 지식 저장과 검색의 관점에서는 최적화되어 있지 않습니다.

Engram은 이러한 gap을 메우기 위해 설계되었습니다.

---

## Engram 모듈 소개

### 핵심 개념

Engram은 N-gram 임베딩을 현대화하여 O(1) 검색 속도를 달성하는 조건부 메모리 모듈입니다.
신경망 계산과 정적 메모리 검색을 결합하여 효율적인 지식 접근을 가능하게 합니다.

### N-gram 임베딩의 현대화

전통적인 N-gram 임베딩 개념을 재해석하여 대규모 언어 모델에 적합하게 변환했습니다.
해시 기반 검색을 통해 상수 시간 복잡도로 관련 정보에 접근합니다.

---

## 기술적 혁신

### O(1) 검색 속도

Engram의 가장 큰 기술적 장점은 검색 시간이 메모리 크기에 관계없이 일정하다는 점입니다.
이는 해시 테이블 기반 설계를 통해 달성됩니다.

### 희소성 할당 문제의 수식화

연구팀은 희소성 할당 문제를 명시적으로 수식화했습니다.
이 과정에서 U자형 스케일링 법칙을 발견했습니다.

### U자형 스케일링 법칙

모델 성능과 희소성 비율 사이에 U자형 관계가 존재합니다.
이는 신경망 계산(MoE)과 정적 메모리(Engram) 사이에 최적의 균형점이 있음을 의미합니다.

---

## 성능 개선 결과

Engram을 적용한 모델은 다양한 벤치마크에서 성능 향상을 보였습니다.

### 영역별 성능 개선

| 벤치마크 | 성능 향상 |
|----------|-----------|
| BBH | +5.0 |
| HumanEval | +3.0 |

### 지식 검색 능력

지식 집약적 작업에서 특히 두드러진 개선을 보였습니다.
외부 지식에 의존하는 질의응답 태스크에서 효과적입니다.

### 추론 및 코딩

놀랍게도 Engram은 지식 검색뿐 아니라 추론과 코딩 영역에서도 성능 향상을 가져왔습니다.
이는 효율적인 지식 접근이 다양한 인지 능력에 긍정적 영향을 미침을 시사합니다.

### 수학 능력

수학적 추론 벤치마크에서도 개선이 관찰되었습니다.
관련 공식이나 정리에 빠르게 접근할 수 있어 문제 해결 속도가 향상됩니다.

---

## 하드웨어 친화적 설계

### 실행 효율성

Engram은 현대 하드웨어 아키텍처를 고려하여 설계되었습니다.
메모리 접근 패턴이 캐시 친화적이며, 병렬 처리에 최적화되어 있습니다.

### 장문맥 처리 능력

Engram을 적용한 모델은 장문맥(Long Context) 처리 능력이 크게 향상되었습니다.
긴 입력 시퀀스에서도 관련 정보를 효율적으로 검색할 수 있습니다.

### 실용적 배포 가능성

하드웨어 친화적 설계 덕분에 실제 프로덕션 환경에서의 배포가 용이합니다.
추가적인 하드웨어 요구사항 없이 기존 인프라에서 활용 가능합니다.

---

## 의의 및 결론

### 새로운 희소성 축의 발견

Engram은 기존의 계산적 희소성(MoE)과 다른 차원의 희소성을 제시합니다.
메모리 기반 희소성이라는 새로운 관점을 통해 LLM 설계의 가능성을 확장했습니다.

### MoE와의 상호보완성

Engram은 MoE를 대체하는 것이 아니라 보완하는 역할을 합니다.
두 접근 방식을 결합하면 더욱 효율적이고 강력한 모델을 구축할 수 있습니다.

### 향후 연구 방향

이 연구는 LLM의 지식 저장 및 검색 메커니즘에 대한 새로운 연구 방향을 제시합니다.
더 효율적인 메모리 구조와 검색 알고리즘에 대한 후속 연구가 기대됩니다.

---

## Reference

- [Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://arxiv.org/abs/2601.07372)
