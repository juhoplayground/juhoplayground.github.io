---
layout: post
title: Open Responses - LLM 상호운용성을 위한 오픈 표준
author: 'Juho'
date: 2026-01-20 00:00:00 +0900
categories: [AI]
tags: [LLM, AI, OpenAI, API]
pin: True
toc: True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }
</style>

## 목차
1. [개요](#개요)
2. [Open Responses란?](#open-responses란)
3. [핵심 목표](#핵심-목표)
4. [주요 특징](#주요-특징)
5. [설계 원칙](#설계-원칙)
6. [참여 커뮤니티](#참여-커뮤니티)
7. [시작 방법](#시작-방법)
8. [결론](#결론)
9. [참고 자료](#참고-자료)

## 개요

현재 LLM 시장에는 OpenAI, Anthropic, Google, Meta 등 다양한 제공자가 존재합니다.
각 제공자마다 API 형식이 다르기 때문에 여러 모델을 사용하려면 각각에 맞는 코드를 작성해야 합니다.
Open Responses는 이러한 문제를 해결하기 위해 등장한 LLM 상호운용성 표준입니다.

## Open Responses란?

### 프로젝트 정의

Open Responses는 다중 LLM 제공자 간 상호운용성을 목표로 하는 오픈소스 규격입니다.
OpenAI Responses API를 기반으로 개발되었으며, 서로 다른 언어 모델 제공자들 간의 호환성을 확보하려는 커뮤니티 프로젝트입니다.

### 해결하려는 문제

각 LLM 제공자가 메시지, 툴 호출, 스트리밍, 멀티모달 입력 등 유사한 구성 요소를 공유하지만 다른 인코딩 방식을 사용합니다.
이로 인해 개발자들은 각 제공자별로 별도의 통합 코드를 작성해야 하는 부담이 있습니다.
Open Responses는 단일 인터페이스로 여러 모델 제공자를 지원하여 이 문제를 해결합니다.

## 핵심 목표

Open Responses의 핵심 목표는 다음과 같습니다.

- 하나의 스키마로 한 번만 정의
- 최소한의 변환으로 여러 공급자에서 실행
- 제공자 간 원활한 전환 지원
- 에이전트 워크플로우 최적화

## 주요 특징

### 공유 스키마

최소한의 변환만으로 다양한 제공자에서 동일한 방식으로 실행할 수 있습니다.
한 번 작성한 코드를 OpenAI, Anthropic, Google 등 여러 제공자에서 재사용할 수 있습니다.

### Items 중심 설계

메시지, 툴 호출, 추론 상태를 같은 단위(Items)로 표현합니다.
이 설계는 에이전트 구축에 최적화되어 있어 복잡한 워크플로우를 쉽게 구현할 수 있습니다.

### 의미론적 이벤트 스트리밍

기존의 텍스트 조각 단위 스트리밍 대신 구조화된 이벤트 기반 처리를 제공합니다.
이를 통해 스트리밍 응답을 더 쉽게 파싱하고 처리할 수 있습니다.

### 확장성

코어 안정성은 유지하면서 제공자별 커스텀 확장을 허용합니다.
표준을 준수하면서도 각 제공자의 고유 기능을 활용할 수 있습니다.

## 설계 원칙

### 다중 공급자 기본 지원

처음부터 여러 LLM 제공자를 지원하도록 설계되었습니다.
특정 제공자에 종속되지 않는 중립적인 표준을 지향합니다.

### 에이전트 워크플로우 친화적

단순한 채팅 완성을 넘어 에이전트 기반 애플리케이션 구축에 최적화되어 있습니다.
툴 호출, 상태 관리, 멀티턴 대화 등을 쉽게 구현할 수 있습니다.

### 단편화 없는 확장성

표준의 핵심은 안정적으로 유지하면서 확장 포인트를 제공합니다.
생태계가 단편화되지 않도록 신중하게 설계되었습니다.

## 참여 커뮤니티

Open Responses에는 다양한 주요 조직들이 참여하고 있습니다.

| 조직 | 역할 |
|------|------|
| OpenRouter | LLM 라우팅 플랫폼 |
| Vercel | 프론트엔드 플랫폼 |
| Hugging Face | AI 모델 허브 |
| LM Studio | 로컬 LLM 실행 도구 |
| Ollama | 로컬 LLM 실행 도구 |
| OpenAI | LLM 제공자 |
| vLLM | LLM 추론 엔진 |

이처럼 LLM 생태계의 다양한 플레이어들이 함께 표준을 만들어가고 있습니다.

## 시작 방법

### 1단계: 핵심 개념 학습

Items, Streaming Events, Tool Use 등 Open Responses의 핵심 개념을 먼저 학습합니다.

### 2단계: OpenAPI 참고자료 검토

공식 OpenAPI 명세를 통해 API 구조와 사용 방법을 파악합니다.

### 3단계: 승인 테스트를 통한 검증

제공되는 승인 테스트를 통해 구현의 호환성을 검증할 수 있습니다.

### 기여 방법

Open Responses는 다양한 영역에서 기여를 환영합니다.

- 스키마 정의 및 개선
- 스트리밍 처리 구현
- 도구 호출 지원
- 테스트 케이스 작성
- 문서화

## 결론

Open Responses는 LLM 생태계의 파편화 문제를 해결하기 위한 중요한 시도입니다.
OpenAI, Vercel, Hugging Face 등 주요 플레이어들이 함께 참여하고 있어 실질적인 표준이 될 가능성이 높습니다.
여러 LLM 제공자를 사용하는 개발자라면 Open Responses의 발전을 주목할 필요가 있습니다.
특히 에이전트 기반 애플리케이션을 개발하는 경우 Items 중심 설계와 의미론적 이벤트 스트리밍이 큰 도움이 될 것입니다.

## 참고 자료

- [Open Responses 공식 사이트](https://www.openresponses.org/)
