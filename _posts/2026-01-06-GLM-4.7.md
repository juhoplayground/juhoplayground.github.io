---
layout: post
title: GLM-4.7 - 코딩 전문 LLM의 새로운 강자
author: 'Juho'
date: 2026-01-06 09:00:00 +0900
categories: [LLM]
tags: [LLM, AI, Python, Benchmark]
pin: True
toc: True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }

</style>

## 목차
1. [GLM-4.7 소개](#glm-47-소개)
2. [주요 특징](#주요-특징)
3. [성능 벤치마크](#성능-벤치마크)
4. [혁신적인 사고 메커니즘](#혁신적인-사고-메커니즘)
5. [사용 방법](#사용-방법)
6. [주의사항 및 한계](#주의사항-및-한계)

## GLM-4.7 소개

GLM-4.7은 Zhipu AI에서 개발한 코딩 전문 대규모 언어 모델입니다.
358B 파라미터의 MoE(Mixture of Experts) 아키텍처를 채택하여 효율적인 추론이 가능하며, MIT 라이선스로 공개되어 상업적 사용도 가능합니다.

**공식 페이지**: [https://z.ai/blog/glm-4.7](https://z.ai/blog/glm-4.7){:target="_blank"}  
**HuggingFace**: [https://huggingface.co/zai-org/GLM-4.7](https://huggingface.co/zai-org/GLM-4.7){:target="_blank"}  
**기술 논문**: [https://arxiv.org/pdf/2508.06471](https://arxiv.org/pdf/2508.06471){:target="_blank"}  

## 주요 특징

GLM-4.7은 네 가지 핵심 영역에서 이전 버전(GLM-4.6) 대비 크게 개선되었습니다.  

### 1. Core Coding (코어 코딩)
다국어 에이전트 코딩과 터미널 작업에서 상당한 성능 향상을 달성했습니다.  

| 벤치마크 | GLM-4.7 | 개선폭 |
|---------|---------|--------|
| SWE-bench | 73.8% | +5.8% |
| SWE-bench Multilingual | 66.7% | +12.9% |
| Terminal Bench 2.0 | 41% | +16.5% |

### 2. Vibe Coding (UI/UX 코딩)
웹페이지와 슬라이드의 UI/UX 품질이 크게 개선되었습니다.  
더 깔끔하고 현대적인 디자인을 생성하며, 정확한 레이아웃의 슬라이드를 만들 수 있습니다.  

### 3. Tool Using (도구 활용)
τ²-Bench와 BrowseComp 벤치마크에서 웹 브라우징 및 다양한 도구 활용 성능이 향상되었습니다.  

### 4. Complex Reasoning (복합 추론)
수학 및 추론 능력이 강화되었습니다.  

| 벤치마크 | GLM-4.7 | 개선폭 |
|---------|---------|--------|
| HLE (Humanity's Last Exam) | 42.8% | +12.4% |
| AIME 2025 | 95.7% | - |
| MMLU-Pro | 84.3% | +1.1% |

## 성능 벤치마크

GLM-4.7은 17개의 추론, 코딩, 에이전트 벤치마크에서 GPT-5, Claude Sonnet 4.5, Gemini 3.0 Pro, DeepSeek-V3.2 등과 비교 평가되었습니다.  

| 벤치마크 | GLM-4.7 | GLM-4.6 | 개선폭 |
|---------|---------|---------|--------|
| MMLU-Pro | 84.3 | 83.2 | +1.1 |
| AIME 2025 | 95.7 | 93.9 | +1.8 |
| HLE (w/ Tools) | 42.8 | 30.4 | +12.4 |
| LiveCodeBench-v6 | 84.9 | 82.8 | +2.1 |
| τ²-Bench | 87.4 | 75.2 | +12.2 |
| SWE-bench | 73.8 | 68.0 | +5.8 |
| Terminal Bench 2.0 | 41.0 | 24.5 | +16.5 |

## 혁신적인 사고 메커니즘

GLM-4.7의 가장 독특한 특징 중 하나는 세 가지 사고(Thinking) 모드를 지원한다는 점입니다.  

### 1. Interleaved Thinking (인터리빙 사고)
모든 응답 및 도구 호출 전에 사고 과정을 수행합니다.  
이를 통해 더 신중하고 정확한 답변을 생성할 수 있습니다.  

### 2. Preserved Thinking (보존된 사고)
다중 턴 대화에서 이전 사고 블록을 유지하여 정보 손실을 감소시킵니다.  
대화의 맥락을 더 잘 이해하고 일관성 있는 응답을 제공합니다.  

### 3. Turn-level Thinking (턴 레벨 사고)
세션 내 개별 턴별로 추론을 제어하여 비용과 지연시간을 최적화합니다.  
필요한 경우에만 깊은 사고를 수행하여 효율성을 높입니다.  

## 사용 방법

GLM-4.7은 다양한 방법으로 사용할 수 있습니다.  

### API를 통한 접근
- **Z.ai API 플랫폼**: [https://docs.z.ai/guides/llm/glm-4.7](https://docs.z.ai/guides/llm/glm-4.7){:target="_blank"}  
- **OpenRouter**: API 키를 통해 접근 가능  

### 코딩 에이전트 통합
다음 코딩 에이전트에서 GLM-4.7을 지원합니다:  
- Claude Code  
- Kilo Code  
- Cline  
- Roo Code  

### 로컬 배포

#### Transformers 라이브러리 사용
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

MODEL_PATH = "zai-org/GLM-4.7"
messages = [{"role": "user", "content": "hello"}]

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_dict=True,
    return_tensors="pt",
)

model = AutoModelForCausalLM.from_pretrained(
    pretrained_model_name_or_path=MODEL_PATH,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

inputs = inputs.to(model.device)
generated_ids = model.generate(**inputs, max_new_tokens=128, do_sample=False)
output_text = tokenizer.decode(generated_ids[0][inputs.input_ids.shape[1]:])
print(output_text)
```

#### vLLM을 통한 배포
```bash
# Docker 이미지 다운로드
docker pull vllm/vllm-openai:nightly

# 또는 pip 설치
pip install -U vllm --pre --index-url https://pypi.org/simple --extra-index-url https://wheels.vllm.ai/nightly

# 서버 실행
vllm serve zai-org/GLM-4.7-FP8 \
     --tensor-parallel-size 4 \
     --speculative-config.method mtp \
     --tool-call-parser glm47 \
     --reasoning-parser glm45 \
     --enable-auto-tool-choice
```

#### SGLang을 통한 배포
```bash
# Docker 이미지 다운로드
docker pull lmsysorg/sglang:dev

# 서버 실행
python3 -m sglang.launch_server \
  --model-path zai-org/GLM-4.7-FP8 \
  --tp-size 8 \
  --tool-call-parser glm47 \
  --reasoning-parser glm45 \
  --speculative-algorithm EAGLE \
  --served-model-name glm-4.7-fp8
```

### 평가 파라미터 설정

#### 기본 설정 (대부분의 작업)
```python
{
    "temperature": 1.0,
    "top_p": 0.95,
    "max_new_tokens": 131072
}
```

#### Terminal Bench, SWE Bench Verified
```python
{
    "temperature": 0.7,
    "top_p": 1.0,
    "max_new_tokens": 16384
}
```

#### τ²-Bench
```python
{
    "temperature": 0,
    "max_new_tokens": 16384
}
```

### Preserved Thinking 활성화 (SGLang)
```json
{
  "chat_template_kwargs": {
    "enable_thinking": true,
    "clear_thinking": false
  }
}
```

## 주의사항 및 한계

### 이용약관 제약
Z.ai의 이용약관에는 몇 가지 제약사항이 있습니다:  
- 경쟁 모델 개발 금지  
- 콘텐츠에 대한 광범위한 사용권 부여  
- 상업적 사용 시 약관 확인 필요  

### 성능 고려사항

#### 로컬 실행 시 속도
Mac Studio 등 일부 환경에서는 토큰 처리 속도가 매우 느릴 수 있습니다.  
고성능 GPU 환경에서의 사용을 권장합니다.  

#### 정치적 민감 주제
중국 기업에서 개발한 모델로, 정치적으로 민감한 주제에서 검열이 있을 수 있습니다.  

### 가격 대비 성능
GLM Coding Plan 구독자는 자동 업그레이드되며, 신규 사용자는 Claude 수준의 모델을 **1/7 가격대**로 이용할 수 있어 비용 효율적입니다.  

## 참고 자료

- [Z.ai 공식 블로그](https://z.ai/blog/glm-4.7){:target="_blank"}
- [HuggingFace 모델 카드](https://huggingface.co/zai-org/GLM-4.7){:target="_blank"}
- [GitHub 저장소](https://github.com/zai-org/GLM-4.5){:target="_blank"}
- [기술 논문 (arXiv)](https://arxiv.org/pdf/2508.06471){:target="_blank"}
- [Z.ai API 문서](https://docs.z.ai/guides/llm/glm-4.7){:target="_blank"}
- [Discord 커뮤니티](https://discord.gg/QR7SARHRxK){:target="_blank"}
