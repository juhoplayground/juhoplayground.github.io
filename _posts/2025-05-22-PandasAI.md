---
layout: post
title: PandasAI를 알아보자
author: 'Juho'
date: 2025-05-22 09:01:00 +0900
categories: [PandasAI]
tags: [PandasAI, Pandas, OpenAI, Ollama, LLM, Python]
pin: True
toc : True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }

</style>

## 목차
1. [PandasAI란?](#pandasai란)
2. [PandasAI with OpenAI](#pandasai-with-openai)
3. [PandasAI with LangCahin](#pandasai-with-langcahin)
4. [PandasAI with OpenSource](#pandasai-with-opensource)
5. [PandasAI로 여러개의 DataFrame 분석하는 방법](#pandasai로-여러개의-dataframe-분석하는-방법)
6. [PandasAI로 멀티턴 대화 질문](#pandasai로-멀티턴-대화-질문)

## PandasAI란?
pandas는 Python으로 데이터 분석을 할 때 가장 널리 사용 되는 라이브러리 중 하나인데 복잡한 쿼리나 분석 로직을 작성하려면 여전히 프로그래밍 지식이 필요하다.  
[PandasAI](https://github.com/sinaptik-ai/pandas-ai){:target="_blank"}는 자연어를 통해 데이터 분석할 수 있게 도와주는 라이브러리이다.  
Enterprise 버전은 [Pandas API KEY](https://pandas-ai.com/){:target="_blank"}를 발급 받아 사용해야하지만 기본적으로는 오픈소스로 사용할 수 있다.  

## PandasAI with OpenAI
```python
from pandasai.llm import OpenAI

llm = OpenAI(api_token = OPENAI_API_KEY, temperature=0.7, model_name='gpt-4o')
df = SmartDataframe(df, config={"llm": llm, "conversational": True, "verbose": True, "enable_cache": False})

code = df.last_code_generated
print(code)

result = df.chat(question)
print(result)
```

## PandasAI with LangCahin
LangChain과 LangGraph로 Agent를 만들려고 한다면 `from pandasai.llm import OpenAI` 대신 `from langchain_openai import ChatOpenAI`을 사용하면 된다.  
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(api_token = OPENAI_API_KEY, temperature=0.7, model_name='gpt-4o')
df = SmartDataframe(df, config={"llm": llm, "conversational": True, "verbose": True, "enable_cache": True})

code = df.last_code_generated
print(code)

result = df.chat(question)
print(result)
```

## PandasAI with OpenSource
하지만 OpenAI를 사용하게 되면 비용이 발생한다는 문제와 데이터 보안이 중요할때는 사용할 수 없다.  
다행히 Local에 설치된 Ollama를 이용해서 오픈소스 LLM을 사용해서도 사용할 수 있다.  
```python
from pandasai.llm.local_llm import LocalLLM

llm = LocalLLM(api_base="http://localhost:11434/v1", temperature=0, model="qwen3:14b")
df = SmartDataframe(df, config={"llm": llm, "conversational": True, "verbose": True, "enable_cache": True})

code = df.last_code_generated
print(code)

result = df.chat(question)
print(result)
```

## PandasAI로 여러개의 DataFrame 분석하는 방법
```python
from pandasai.llm.local_llm import LocalLLM
from pandasai import SmartDatalake

llm = LocalLLM(api_base="http://localhost:11434/v1", temperature=0, model="qwen3:14b")
lake = SmartDatalake([df1, df2], config={"llm": llm, "conversational": True, "verbose": True, "enable_cache": True})

result = lake.chat(question)
print(result)
```


## PandasAI로 멀티턴 대화 질문
```
from pandasai.llm.local_llm import LocalLLM
from pandasai import Agent

agent = Agent([df1, df2], config={"llm": llm, "conversational": True, "verbose": True, "enable_cache": True})
agent.chat(question)
```

agent가 질문에 대한 답변이 만족스럽지 않을 경우 추가 질문을 할 수 있습니다.  
```python
agent.clarification_questions(question)
```

agent가 왜 이렇게 답변을 생성했는지 관련해서 설명을 시킬 수 있습니다.  
```python
result = agent.chat(question)
explanation = agent.explain()
print(result)
print(explanation)
```

애매한 질문을 입력했을 때 조금 더 포괄적인 답변을 받으려면 질문을 변경해서 agent한테 질문을 할 수 있습니다.  
```
question2 = agent.rephrase_query(question)
result = agent.chat(question2)
print(result)
```


---  

PandasAI도 만능은 아니고 잘못된 질문이나 애매한 질문에는 틀린 결과를 낼 수 있다.  
그리고 큰 데이터셋에서는 응답 속도가 조금 느린편이였다.  

