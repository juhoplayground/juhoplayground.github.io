---
layout: post
title: Ollama 사용 방법
author: 'Juho'
date: 2025-04-21 09:01:00 +0900
categories: [Ollama]
tags: [Ollama, LLM, Python]
pin: True
toc : True
---

<style>
  th{
    font-weight: bold;
    text-align: center;
    background-color: white;
  }
  td{
    background-color: white;
  }

</style>

## 목차
1. [Ollama](#ollama)
2. [Ollama 다운로드 및 설치 방법](#ollama-다운로드-및-설치-방법)
3. [모델 다운로드 및 실행](#모델-다운로드-및-실행)

## Ollama
Ollama는 오픈소스 LLM을 로컬 PC에서 쉽게 실행할 수 있게 해주는 도구  
모델 가중치, 설정, 데이터셋을 하나의 패키지로 묶어서 Modelfile로 관리  

## Ollama 다운로드 및 설치 방법
리눅스에서 Ollama를 다운로드하고 설치하는 방법은 간단  
```bash
wget https://ollama.ai/install.sh
sh ./install.sh
```

2개 명령어를 실행하면 다운로드 및 설치가 완료됨 

### 동작 확인  
정상적으로 동작하는지 확인하기 위해서 `sudo systemctl status ollama` 입력  
```bash
(artience_llm) ubuntu@ip-172-31-4-41:~/artience_llm$ sudo systemctl status ollama
● ollama.service - Ollama Service
     Loaded: loaded (/etc/systemd/system/ollama.service; enabled; preset: enabled)
     Active: active (running) since Mon 2025-04-21 13:35:26 KST; 2s ago
   Main PID: 278302 (ollama)
      Tasks: 9 (limit: 76092)
     Memory: 16.4M (peak: 26.6M)
        CPU: 245ms
     CGroup: /system.slice/ollama.service
             └─278302 /usr/local/bin/ollama serve

Apr 21 13:35:26 ip-172-31-4-41 systemd[1]: Started ollama.service - Ollama Service.
```
정상적으로 동작하는 것을 확인할 수 있음  

### 명령어 확인 
명령어 확인하기 위해서 `ollama` 라고 터미널에 입력  
```bash
Usage:
  ollama [flags]
  ollama [command]

Available Commands:
  serve       Start ollama
  create      Create a model from a Modelfile
  show        Show information for a model
  run         Run a model
  stop        Stop a running model
  pull        Pull a model from a registry
  push        Push a model to a registry
  list        List models
  ps          List running models
  cp          Copy a model
  rm          Remove a model
  help        Help about any command

Flags:
  -h, --help      help for ollama
  -v, --version   Show version information

Use "ollama [command] --help" for more information about a command.
```

그럼 이러한 형태로 확인할 수 있음

###  바인딩 주소 변경
처음 ollama가 설치되면 `127.0.0.1:11434`로 띄워져있는 것을 확인할 수 있음  
외부 호스트에서 Ollama에 연결하려면 바인딩 주소를 0.0.0.0:11434 으로 변경해야 함  
`sudo vi /etc/systemd/system/ollama.service`를 입력하여  
Service 영역에 `Environment="OLLAMA_HOST=0.0.0.0"`를 추가  
아래의 명령어로 ollama 재실행하고 확인  
```bash
sudo systemctl daemon-reload
sudo systemctl restart ollama
```

## 모델 다운로드 및 실행
모델 실행없이 가중치만 다운로드 하려면
```bash
ollama pull <모델이름>
```

모델 가중치 다운로드 해서 터미널에서 사용자 입력을 받을 수 있게 하려면  
```bash
ollama run <모델 이름>
```


---  

Ollama는 HuggingFace에 비해서 많은 모델이 없지만 로컬 환경에서 실행하기는 훨씬 더 편리한 것 같음

